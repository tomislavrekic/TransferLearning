{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Dropout\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input, MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9412608/9406464 [==============================] - 11s 1us/step\n"
     ]
    }
   ],
   "source": [
    "base_model=MobileNetV2(weights='imagenet',include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x)\n",
    "x=Dropout(0.3)(x)\n",
    "x=Dense(1024,activation='relu')(x)\n",
    "x=Dropout(0.3)(x)\n",
    "x=Dense(512,activation='relu')(x)\n",
    "x=Dropout(0.5)(x)\n",
    "preds=Dense(51,activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=Model(inputs=base_model.input,outputs=preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 Conv1_pad\n",
      "2 Conv1\n",
      "3 bn_Conv1\n",
      "4 Conv1_relu\n",
      "5 expanded_conv_depthwise\n",
      "6 expanded_conv_depthwise_BN\n",
      "7 expanded_conv_depthwise_relu\n",
      "8 expanded_conv_project\n",
      "9 expanded_conv_project_BN\n",
      "10 block_1_expand\n",
      "11 block_1_expand_BN\n",
      "12 block_1_expand_relu\n",
      "13 block_1_pad\n",
      "14 block_1_depthwise\n",
      "15 block_1_depthwise_BN\n",
      "16 block_1_depthwise_relu\n",
      "17 block_1_project\n",
      "18 block_1_project_BN\n",
      "19 block_2_expand\n",
      "20 block_2_expand_BN\n",
      "21 block_2_expand_relu\n",
      "22 block_2_depthwise\n",
      "23 block_2_depthwise_BN\n",
      "24 block_2_depthwise_relu\n",
      "25 block_2_project\n",
      "26 block_2_project_BN\n",
      "27 block_2_add\n",
      "28 block_3_expand\n",
      "29 block_3_expand_BN\n",
      "30 block_3_expand_relu\n",
      "31 block_3_pad\n",
      "32 block_3_depthwise\n",
      "33 block_3_depthwise_BN\n",
      "34 block_3_depthwise_relu\n",
      "35 block_3_project\n",
      "36 block_3_project_BN\n",
      "37 block_4_expand\n",
      "38 block_4_expand_BN\n",
      "39 block_4_expand_relu\n",
      "40 block_4_depthwise\n",
      "41 block_4_depthwise_BN\n",
      "42 block_4_depthwise_relu\n",
      "43 block_4_project\n",
      "44 block_4_project_BN\n",
      "45 block_4_add\n",
      "46 block_5_expand\n",
      "47 block_5_expand_BN\n",
      "48 block_5_expand_relu\n",
      "49 block_5_depthwise\n",
      "50 block_5_depthwise_BN\n",
      "51 block_5_depthwise_relu\n",
      "52 block_5_project\n",
      "53 block_5_project_BN\n",
      "54 block_5_add\n",
      "55 block_6_expand\n",
      "56 block_6_expand_BN\n",
      "57 block_6_expand_relu\n",
      "58 block_6_pad\n",
      "59 block_6_depthwise\n",
      "60 block_6_depthwise_BN\n",
      "61 block_6_depthwise_relu\n",
      "62 block_6_project\n",
      "63 block_6_project_BN\n",
      "64 block_7_expand\n",
      "65 block_7_expand_BN\n",
      "66 block_7_expand_relu\n",
      "67 block_7_depthwise\n",
      "68 block_7_depthwise_BN\n",
      "69 block_7_depthwise_relu\n",
      "70 block_7_project\n",
      "71 block_7_project_BN\n",
      "72 block_7_add\n",
      "73 block_8_expand\n",
      "74 block_8_expand_BN\n",
      "75 block_8_expand_relu\n",
      "76 block_8_depthwise\n",
      "77 block_8_depthwise_BN\n",
      "78 block_8_depthwise_relu\n",
      "79 block_8_project\n",
      "80 block_8_project_BN\n",
      "81 block_8_add\n",
      "82 block_9_expand\n",
      "83 block_9_expand_BN\n",
      "84 block_9_expand_relu\n",
      "85 block_9_depthwise\n",
      "86 block_9_depthwise_BN\n",
      "87 block_9_depthwise_relu\n",
      "88 block_9_project\n",
      "89 block_9_project_BN\n",
      "90 block_9_add\n",
      "91 block_10_expand\n",
      "92 block_10_expand_BN\n",
      "93 block_10_expand_relu\n",
      "94 block_10_depthwise\n",
      "95 block_10_depthwise_BN\n",
      "96 block_10_depthwise_relu\n",
      "97 block_10_project\n",
      "98 block_10_project_BN\n",
      "99 block_11_expand\n",
      "100 block_11_expand_BN\n",
      "101 block_11_expand_relu\n",
      "102 block_11_depthwise\n",
      "103 block_11_depthwise_BN\n",
      "104 block_11_depthwise_relu\n",
      "105 block_11_project\n",
      "106 block_11_project_BN\n",
      "107 block_11_add\n",
      "108 block_12_expand\n",
      "109 block_12_expand_BN\n",
      "110 block_12_expand_relu\n",
      "111 block_12_depthwise\n",
      "112 block_12_depthwise_BN\n",
      "113 block_12_depthwise_relu\n",
      "114 block_12_project\n",
      "115 block_12_project_BN\n",
      "116 block_12_add\n",
      "117 block_13_expand\n",
      "118 block_13_expand_BN\n",
      "119 block_13_expand_relu\n",
      "120 block_13_pad\n",
      "121 block_13_depthwise\n",
      "122 block_13_depthwise_BN\n",
      "123 block_13_depthwise_relu\n",
      "124 block_13_project\n",
      "125 block_13_project_BN\n",
      "126 block_14_expand\n",
      "127 block_14_expand_BN\n",
      "128 block_14_expand_relu\n",
      "129 block_14_depthwise\n",
      "130 block_14_depthwise_BN\n",
      "131 block_14_depthwise_relu\n",
      "132 block_14_project\n",
      "133 block_14_project_BN\n",
      "134 block_14_add\n",
      "135 block_15_expand\n",
      "136 block_15_expand_BN\n",
      "137 block_15_expand_relu\n",
      "138 block_15_depthwise\n",
      "139 block_15_depthwise_BN\n",
      "140 block_15_depthwise_relu\n",
      "141 block_15_project\n",
      "142 block_15_project_BN\n",
      "143 block_15_add\n",
      "144 block_16_expand\n",
      "145 block_16_expand_BN\n",
      "146 block_16_expand_relu\n",
      "147 block_16_depthwise\n",
      "148 block_16_depthwise_BN\n",
      "149 block_16_depthwise_relu\n",
      "150 block_16_project\n",
      "151 block_16_project_BN\n",
      "152 Conv_1\n",
      "153 Conv_1_bn\n",
      "154 out_relu\n",
      "155 global_average_pooling2d\n",
      "156 dense\n",
      "157 dropout\n",
      "158 dense_1\n",
      "159 dropout_1\n",
      "160 dense_2\n",
      "161 dropout_2\n",
      "162 dense_3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,layer in enumerate(model.layers):\n",
    "  print(i,layer.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:156]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[156:]:\n",
    "    layer.trainable=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5238 images belonging to 51 classes.\n",
      "Found 562 images belonging to 51 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "IMG_SIZE = 224\n",
    "DATADIR = \"/home/rekca/Datasets/Caltech/CaltechAnimals (copy 1)/256_ObjectCategories/\"\n",
    "'''\n",
    "train_datagen = ImageDataGenerator( rotation_range=60, width_shift_range=0.1,\n",
    "                            height_shift_range=0.1, shear_range=0.2,\n",
    "                            zoom_range=0.2, horizontal_flip = True,\n",
    "                           fill_mode ='nearest', rescale=1./255, validation_split=0.1)\n",
    "'''\n",
    "\n",
    "\"\"\"\n",
    "train_datagen = ImageDataGenerator( validation_split=0.1,preprocessing_function=preprocess_input,\n",
    "                                  horizontal_flip=True,rotation_range=40, width_shift_range=0.1,\n",
    "                                   height_shift_range=0.1, shear_range=0.2,zoom_range=0.2,fill_mode ='nearest')\n",
    "\"\"\"\n",
    "train_datagen = ImageDataGenerator( validation_split=0.1,preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        DATADIR,  \n",
    "        target_size=(IMG_SIZE, IMG_SIZE),  \n",
    "        color_mode='rgb',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True, \n",
    "        subset='training') \n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "        DATADIR,\n",
    "        target_size=(IMG_SIZE,IMG_SIZE),\n",
    "        color_mode='rgb',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle = True,\n",
    "        subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"mobile2.{epoch:02d}-{val_acc:.2f}.hdf5\" \n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True,\n",
    "                            mode='min', period=1)\n",
    "\n",
    "callBacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "187/187 [==============================] - 470s 3s/step - loss: 0.7664 - acc: 0.7927 - val_loss: 0.5294 - val_acc: 0.8562\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - 465s 2s/step - loss: 0.6472 - acc: 0.8158 - val_loss: 0.4667 - val_acc: 0.8896\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - 467s 2s/step - loss: 0.5696 - acc: 0.8439 - val_loss: 0.4922 - val_acc: 0.8708\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - 473s 3s/step - loss: 0.4853 - acc: 0.8649 - val_loss: 0.5416 - val_acc: 0.8688\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - 480s 3s/step - loss: 0.4823 - acc: 0.8693 - val_loss: 0.5127 - val_acc: 0.8792\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - 453s 2s/step - loss: 0.4144 - acc: 0.8889 - val_loss: 0.4202 - val_acc: 0.8875\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - 442s 2s/step - loss: 0.3796 - acc: 0.8944 - val_loss: 0.5323 - val_acc: 0.8750\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - 423s 2s/step - loss: 0.3688 - acc: 0.9010 - val_loss: 0.5130 - val_acc: 0.8771\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - 425s 2s/step - loss: 0.3265 - acc: 0.9123 - val_loss: 0.7104 - val_acc: 0.8583\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 422s 2s/step - loss: 0.3012 - acc: 0.9146 - val_loss: 0.6269 - val_acc: 0.8646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe30f2a4a58>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=6000 // batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=500 // batch_size, callbacks = callBacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"MobileNetV2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 1132 variables.\n",
      "INFO:tensorflow:Converted 1132 variables to const ops.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27780024"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = tf.contrib.lite.TFLiteConverter.from_keras_model_file('mobile2.04-0.33.hdf5')\n",
    "tflite_model = converter.convert()\n",
    "open(\"Mobile2.tflite\", \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '007.bat', 1: '009.bear', 2: '024.butterfly', 3: '028.camel', 4: '034.centipede', 5: '038.chimp', 6: '040.cockroach', 7: '049.cormorant', 8: '052.crab-101', 9: '056.dog', 10: '057.dolphin-101', 11: '060.duck', 12: '064.elephant-101', 13: '065.elk', 14: '080.frog', 15: '084.giraffe', 16: '085.goat', 17: '087.goldfish', 18: '089.goose', 19: '090.gorilla', 20: '093.grasshopper', 21: '100.hawksbill-101', 22: '105.horse', 23: '106.horseshoe-crab', 24: '111.house-fly', 25: '113.hummingbird', 26: '114.ibis-101', 27: '116.iguana', 28: '121.kangaroo-101', 29: '124.killer-whale', 30: '129.leopards-101', 31: '134.llama-101', 32: '150.octopus', 33: '151.ostrich', 34: '152.owl', 35: '158.penguin', 36: '159.people', 37: '164.porcupine', 38: '166.praying-mantis', 39: '168.raccoon', 40: '179.scorpion-101', 41: '186.skunk', 42: '189.snail', 43: '190.snake', 44: '198.spider', 45: '201.starfish-101', 46: '207.swan', 47: '228.triceratops', 48: '230.trilobite-101', 49: '250.zebra', 50: '256.toad'}\n"
     ]
    }
   ],
   "source": [
    "label_map = (validation_generator.class_indices)\n",
    "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
    "\n",
    "print(label_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
